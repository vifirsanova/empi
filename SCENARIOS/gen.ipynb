{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16QLrpcg0xCWcL6QH-z7Tf2Y_sYvqwd08",
      "authorship_tag": "ABX9TyPGlxvVBHLk/hPfTFgJEnuK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/empi/blob/main/SCENARIOS/gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class Encoder():\n",
        "  def __init__(self, block, prompt):\n",
        "    self.prompt = prompt\n",
        "    self.block = block\n",
        "  def tokenizer(self):\n",
        "    tokenized_prompt = self.prompt.split()\n",
        "    return tokenized_prompt\n",
        "  def cypher(self, tokenized_prompt):\n",
        "    cyphered_prompt = tokenized_prompt\n",
        "    return cyphered_prompt\n",
        "  def get_info(self, cyphered_prompt):\n",
        "    prompt_info = cyphered_prompt\n",
        "    return prompt_info\n",
        "  def update_blockchain(self, current_block, cyphered_prompt, prompt_info):\n",
        "    current_block = [cyphered_prompt, prompt_info]\n",
        "    return current_block\n",
        "  def save_block(self):\n",
        "    tokenized_prompt = self.tokenizer()\n",
        "    cyphered_prompt = self.cypher(tokenized_prompt)\n",
        "    prompt_info = self.get_info(cyphered_prompt)\n",
        "    current_block = self.update_blockchain(self.block, cyphered_prompt, prompt_info)\n",
        "    user_block = current_block # to json\n",
        "    return user_block\n",
        "\n",
        "class Decoder():\n",
        "  def __init__(self, current_block):\n",
        "    self.current_block = current_block\n",
        "  def block2chain(self):\n",
        "    vectorized_block = self.current_block\n",
        "    return vectorized_block\n",
        "  def language_model(self, vectorized_block):\n",
        "    lm_outputs = [vectorized_block, vectorized_block]\n",
        "    return lm_outputs\n",
        "  def privacy_consistency_check(self, lm_outputs):\n",
        "    privacy_scores = {}\n",
        "    consistency_scores = {}\n",
        "    for output in lm_outputs[0][0]:\n",
        "      privacy_scores[output] = 1\n",
        "      consistency_scores[output] = 1\n",
        "    return privacy_scores, consistency_scores\n",
        "  def human_evaluation(self, lm_outputs):\n",
        "    preference_scores = {}\n",
        "    for output in lm_outputs[0][0]:\n",
        "      preference_scores[output] = 1\n",
        "    return preference_scores\n",
        "  def argmax(self, privacy_scores, consistency_scores, preference_scores):\n",
        "    best_model_response = privacy_scores, consistency_scores, preference_scores\n",
        "    return best_model_response\n",
        "  def model_output(self):\n",
        "    vectorized_block = self.block2chain()\n",
        "    lm_outputs = self.language_model(vectorized_block)\n",
        "    privacy_scores, consistency_scores = self.privacy_consistency_check(lm_outputs)\n",
        "    preference_scores = self.human_evaluation(lm_outputs)\n",
        "    best_model_response = self.argmax(privacy_scores, consistency_scores, preference_scores)\n",
        "    outputs = best_model_response # text only\n",
        "    return best_model_response\n",
        "\n",
        "def modeling(block, prompt):\n",
        "  current_block = Encoder(block, prompt).save_block()\n",
        "  result = Decoder(current_block).model_output()\n",
        "  return result\n",
        "\n",
        "#!wget -O block.json https://raw.githubusercontent.com/vifirsanova/empi/main/SCENARIOS/%D0%90%D0%BD%D1%8F_block(1).json\n",
        "\n",
        "#with open('block.json', 'r') as f:\n",
        "#  block = json.load(f)\n",
        "\n",
        "modeling()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Z1MgvjILLy",
        "outputId": "c2f15210-7039-4dea-c141-897e2300c11d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "check the function\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'check': 1, 'the': 1, 'function': 1},\n",
              " {'check': 1, 'the': 1, 'function': 1},\n",
              " {'check': 1, 'the': 1, 'function': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}
