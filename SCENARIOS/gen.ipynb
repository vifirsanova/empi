{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16QLrpcg0xCWcL6QH-z7Tf2Y_sYvqwd08",
      "authorship_tag": "ABX9TyMNmqaZGkbn5Hgp95/k4awD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/empi/blob/main/SCENARIOS/gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class Encoder():\n",
        "  \"\"\"\n",
        "  Класс для энкодера:\n",
        "  - токенизирует промпт\n",
        "  - шифрует персональные данные с помощью поиска по графу\n",
        "  - ищет релевантную информацию в графе знаний\n",
        "  - формирует блок\n",
        "  - обновляет цепочку блоков\n",
        "  - сохраняет информацию в *.json для интерпретации результатов генерации\n",
        "  \"\"\"\n",
        "  def __init__(self, block, prompt):\n",
        "    self.prompt = prompt\n",
        "    self.block = block\n",
        "  def tokenizer(self):\n",
        "    tokenized_prompt = self.prompt.split()\n",
        "    return tokenized_prompt\n",
        "  def cypher(self, tokenized_prompt):\n",
        "    cyphered_prompt = tokenized_prompt\n",
        "    return cyphered_prompt\n",
        "  def get_info(self, cyphered_prompt):\n",
        "    prompt_info = cyphered_prompt\n",
        "    return prompt_info\n",
        "  def update_blockchain(self, current_block, cyphered_prompt, prompt_info):\n",
        "    current_block = [cyphered_prompt, prompt_info]\n",
        "    return current_block\n",
        "  def save_block(self):\n",
        "    tokenized_prompt = self.tokenizer()\n",
        "    cyphered_prompt = self.cypher(tokenized_prompt)\n",
        "    prompt_info = self.get_info(cyphered_prompt)\n",
        "    current_block = self.update_blockchain(self.block, cyphered_prompt, prompt_info)\n",
        "    user_block = current_block # to json\n",
        "    return user_block\n",
        "\n",
        "class Decoder():\n",
        "  \"\"\"\n",
        "  Класс для декодера:\n",
        "  - принимает на вход блок, сформированный энкодером\n",
        "  - преобразует блок к виду вектора\n",
        "  - инициализирует вектор как префикс для обусловленной генерации с LLM\n",
        "  - оценивает выдачу LLM по критериям \"приватность\" и \"информативность\" на основе графа\n",
        "  - оценивает выдачу LLM на основе отзывов людей\n",
        "  - выводит взвешенное среднее 3-х оценок для выбора лучшего варианта генерации\n",
        "  - возвращает выдачу с наибольшей финальной оценкой\n",
        "  \"\"\"\n",
        "  def __init__(self, current_block):\n",
        "    self.current_block = current_block\n",
        "  def block2chain(self):\n",
        "    vectorized_block = self.current_block\n",
        "    return vectorized_block\n",
        "  def language_model(self, vectorized_block):\n",
        "    lm_outputs = [vectorized_block, vectorized_block]\n",
        "    return lm_outputs\n",
        "  def privacy_consistency_check(self, lm_outputs):\n",
        "    privacy_scores = {}\n",
        "    consistency_scores = {}\n",
        "    for output in lm_outputs[0][0]:\n",
        "      privacy_scores[output] = 1\n",
        "      consistency_scores[output] = 1\n",
        "    return privacy_scores, consistency_scores\n",
        "  def human_evaluation(self, lm_outputs):\n",
        "    preference_scores = {}\n",
        "    for output in lm_outputs[0][0]:\n",
        "      preference_scores[output] = 1\n",
        "    return preference_scores\n",
        "  def argmax(self, privacy_scores, consistency_scores, preference_scores):\n",
        "    best_model_response = privacy_scores, consistency_scores, preference_scores\n",
        "    return best_model_response\n",
        "  def model_output(self):\n",
        "    vectorized_block = self.block2chain()\n",
        "    lm_outputs = self.language_model(vectorized_block)\n",
        "    privacy_scores, consistency_scores = self.privacy_consistency_check(lm_outputs)\n",
        "    preference_scores = self.human_evaluation(lm_outputs)\n",
        "    best_model_response = self.argmax(privacy_scores, consistency_scores, preference_scores)\n",
        "    outputs = best_model_response # text only\n",
        "    return best_model_response\n",
        "\n",
        "class Model():\n",
        "  \"\"\"\n",
        "  Класс для модели:\n",
        "  - обрабатывает блок и промпт на данной итерации\n",
        "  - энкодер принимает текущий блок и промпт и возвращает блок\n",
        "  - декодер принимает блок энкодера и возвращает лучшую выдачу для данного блока\n",
        "  - модель возвращает строку - лучшую генерацию декодера\n",
        "  \"\"\"\n",
        "  def __init__(self, block, prompt):\n",
        "    self.block = block\n",
        "    self.prompt = prompt\n",
        "\n",
        "  def answer(self):\n",
        "    current_block = Encoder(self.block, self.prompt).save_block()\n",
        "    result = Decoder(current_block).model_output()\n",
        "    return result\n",
        "\n",
        "!wget -O block.json https://raw.githubusercontent.com/vifirsanova/empi/main/SCENARIOS/%D0%90%D0%BD%D1%8F_block\\(1\\).json\n",
        "\n",
        "# загружаем блок с предыдущей итерации\n",
        "with open('block.json', 'r') as f:\n",
        "  block = json.load(f)\n",
        "\n",
        "# принимаем промпт пользователя\n",
        "prompt = 'checking the function'\n",
        "\n",
        "# инициализируем модель\n",
        "model = Model(block, prompt)\n",
        "\n",
        "# генерируем ответ (KB IR + Blockchain + Weighted LLM Generation)\n",
        "model.answer()"
      ],
      "metadata": {
        "id": "H6Z1MgvjILLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c56bb59-b63c-4c9c-860a-78368d366567"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-16 17:28:13--  https://raw.githubusercontent.com/vifirsanova/empi/main/SCENARIOS/%D0%90%D0%BD%D1%8F_block(1).json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1250 (1.2K) [text/plain]\n",
            "Saving to: ‘block.json’\n",
            "\n",
            "\rblock.json            0%[                    ]       0  --.-KB/s               \rblock.json          100%[===================>]   1.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-16 17:28:13 (20.6 MB/s) - ‘block.json’ saved [1250/1250]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'checking': 1, 'the': 1, 'function': 1},\n",
              " {'checking': 1, 'the': 1, 'function': 1},\n",
              " {'checking': 1, 'the': 1, 'function': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}