{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16QLrpcg0xCWcL6QH-z7Tf2Y_sYvqwd08",
      "authorship_tag": "ABX9TyN5oVC8TNhvSwXUSUkOC3tZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/empi/blob/main/SCENARIOS/gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class Encoder():\n",
        "  def __init__(self, block, prompt):\n",
        "    self.prompt = prompt\n",
        "    self.block = block\n",
        "  def tokenizer(self):\n",
        "    tokenized_prompt = self.prompt.split()\n",
        "    return tokenized_prompt\n",
        "  def cypher(self, tokenized_prompt):\n",
        "    cyphered_prompt = tokenized_prompt\n",
        "    return cyphered_prompt\n",
        "  def get_info(self, cyphered_prompt):\n",
        "    prompt_info = cyphered_prompt\n",
        "    return prompt_info\n",
        "  def update_blockchain(self, current_block, cyphered_prompt, prompt_info):\n",
        "    current_block = [cyphered_prompt, prompt_info]\n",
        "    return current_block\n",
        "  def save_block(self):\n",
        "    tokenized_prompt = self.tokenizer()\n",
        "    cyphered_prompt = self.cypher(tokenized_prompt)\n",
        "    prompt_info = self.get_info(cyphered_prompt)\n",
        "    current_block = self.update_blockchain(self.block, cyphered_prompt, prompt_info)\n",
        "    user_block = current_block # to json\n",
        "    return user_block\n",
        "\n",
        "class Decoder():\n",
        "  def __init__(self, current_block):\n",
        "    self.current_block = current_block\n",
        "  def block2chain(self):\n",
        "    vectorized_block = self.current_block\n",
        "    return vectorized_block\n",
        "  def language_model(self, vectorized_block):\n",
        "    lm_outputs = [vectorized_block, vectorized_block]\n",
        "    return lm_outputs\n",
        "  def privacy_consistency_check(self, lm_outputs):\n",
        "    privacy_scores = {}\n",
        "    consistency_scores = {}\n",
        "    for output in lm_outputs[0][0]:\n",
        "      privacy_scores[output] = 1\n",
        "      consistency_scores[output] = 1\n",
        "    return privacy_scores, consistency_scores\n",
        "  def human_evaluation(self, lm_outputs):\n",
        "    preference_scores = {}\n",
        "    for output in lm_outputs[0][0]:\n",
        "      preference_scores[output] = 1\n",
        "    return preference_scores\n",
        "  def argmax(self, privacy_scores, consistency_scores, preference_scores):\n",
        "    best_model_response = privacy_scores, consistency_scores, preference_scores\n",
        "    return best_model_response\n",
        "  def model_output(self):\n",
        "    vectorized_block = self.block2chain()\n",
        "    lm_outputs = self.language_model(vectorized_block)\n",
        "    privacy_scores, consistency_scores = self.privacy_consistency_check(lm_outputs)\n",
        "    preference_scores = self.human_evaluation(lm_outputs)\n",
        "    best_model_response = self.argmax(privacy_scores, consistency_scores, preference_scores)\n",
        "    outputs = best_model_response # text only\n",
        "    return best_model_response\n",
        "\n",
        "class Model():\n",
        "  def __init__(self, block, prompt):\n",
        "    self.block = block\n",
        "    self.prompt = prompt\n",
        "\n",
        "  def answer(self):\n",
        "    current_block = Encoder(self.block, self.prompt).save_block()\n",
        "    result = Decoder(current_block).model_output()\n",
        "    return result\n",
        "\n",
        "#!wget -O block.json https://raw.githubusercontent.com/vifirsanova/empi/main/SCENARIOS/%D0%90%D0%BD%D1%8F_block(1).json\n",
        "\n",
        "#with open('block.json', 'r') as f:\n",
        "#  block = json.load(f)\n",
        "\n",
        "model = Model(block, prompt)\n",
        "model.answer()"
      ],
      "metadata": {
        "id": "H6Z1MgvjILLy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}